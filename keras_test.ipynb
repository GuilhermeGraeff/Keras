{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a112b5c-5395-4654-9025-bca2cf33f85f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a' 'x']\n",
      " ['a' 'x']\n",
      " ['a' 'x']\n",
      " ['b' 'y']]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([[\"a\", \"x\"],\n",
    "                   [\"a\", \"x\"],\n",
    "                   [\"a\", np.nan],\n",
    "                   [\"b\", \"y\"]], dtype=\"category\")\n",
    "\n",
    "imp = SimpleImputer(strategy=\"most_frequent\")\n",
    "print(imp.fit_transform(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f1229953-2cba-4aad-b058-b3b68557a74b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fda6c091-b7c7-4baf-97ba-45ffaacadc05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[5 2]\n",
      " [1 3]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Constant Trnsors\n",
    "x = tf.constant([[5, 2], [1, 3]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3aff3936-fb13-4d35-8b47-7f112adad3b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 2],\n",
       "       [1, 3]], dtype=int32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6da43259-d46c-4eec-a843-98d2ec0d7255",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aqui o dtype <dtype: 'int32'>\n",
      "Aqui o shape (2, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Aqui o dtype\", x.dtype)\n",
    "print(\"Aqui o shape\", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0f40f13d-8329-417b-bbb9-37cc55817014",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]], shape=(4, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]], shape=(4, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "ones = tf.ones(shape=(4,4))\n",
    "zeros = tf.zeros(shape=(4,4))\n",
    "print(ones)\n",
    "print(zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "47f25113-6b75-4283-a0db-1c93de2be0fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.40754527 -0.98947674  0.02574982  0.7577073 ]\n",
      " [ 0.19375835  0.32681334 -0.41315946 -1.0953197 ]\n",
      " [-0.12834647  1.1091732  -0.6187683  -0.18008278]\n",
      " [ 0.00244933  0.6929563   1.2824197  -0.13641357]], shape=(4, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[2 7 2 1]\n",
      " [8 7 6 2]\n",
      " [9 9 7 1]\n",
      " [5 5 1 9]], shape=(4, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "random_floats = tf.random.normal(shape=(4,4), mean=0.0, stddev=1.0)\n",
    "random_ints = tf.random.uniform(shape=(4,4), minval=0, maxval=10, dtype=\"int32\")\n",
    "\n",
    "print(random_floats)\n",
    "print(random_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cad2e9cb-3ca9-414b-ad07-cd8a4eb50d05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 1.1551441   0.60057086 -0.10289282 -0.4653137 ]\n",
      " [ 0.8708803   1.2960113  -0.10215749  0.3286005 ]\n",
      " [ 0.10687343 -0.6880179  -0.91159374 -1.654839  ]\n",
      " [-0.19849642 -0.30487847  0.20681468 -0.36508146]], shape=(4, 4), dtype=float32)\n",
      "<tf.Variable 'Variable:0' shape=(4, 4) dtype=float32, numpy=\n",
      "array([[ 1.1551441 ,  0.60057086, -0.10289282, -0.4653137 ],\n",
      "       [ 0.8708803 ,  1.2960113 , -0.10215749,  0.3286005 ],\n",
      "       [ 0.10687343, -0.6880179 , -0.91159374, -1.654839  ],\n",
      "       [-0.19849642, -0.30487847,  0.20681468, -0.36508146]],\n",
      "      dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "# Variable tensors\n",
    "initial_value = tf.random.normal(shape=(4,4))\n",
    "a = tf.Variable(initial_value)\n",
    "print(initial_value)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "51c0f4cf-60e3-4bc1-96f5-51440fa92229",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before assign new values to 'a'\n",
      "<tf.Variable 'Variable:0' shape=(4, 4) dtype=float32, numpy=\n",
      "array([[ 1.1551441 ,  0.60057086, -0.10289282, -0.4653137 ],\n",
      "       [ 0.8708803 ,  1.2960113 , -0.10215749,  0.3286005 ],\n",
      "       [ 0.10687343, -0.6880179 , -0.91159374, -1.654839  ],\n",
      "       [-0.19849642, -0.30487847,  0.20681468, -0.36508146]],\n",
      "      dtype=float32)>\n",
      "\n",
      "\n",
      "\n",
      "After assign new values to 'a'\n",
      "<tf.Variable 'Variable:0' shape=(4, 4) dtype=float32, numpy=\n",
      "array([[ 1.68362212e+00, -3.92901152e-01,  9.91967797e-01,\n",
      "        -4.91323382e-01],\n",
      "       [-1.09751113e-01,  1.11285925e-01,  1.55584253e-02,\n",
      "         1.09220529e+00],\n",
      "       [-1.11858058e+00, -8.80956352e-01,  4.98265140e-02,\n",
      "         2.27205515e+00],\n",
      "       [ 4.69782382e-01,  9.08876896e-01,  2.15615658e-03,\n",
      "         1.57217789e+00]], dtype=float32)>\n",
      "+\n",
      "tf.Tensor(\n",
      "[[ 1.6180454  -1.7764399   0.7954538   1.8387125 ]\n",
      " [ 2.0276284  -0.32959095  0.9455347   0.09089289]\n",
      " [-0.32055    -0.04092838  1.4108615   1.2921307 ]\n",
      " [-0.9135122  -1.5844263   0.29941848 -0.37129214]], shape=(4, 4), dtype=float32)\n",
      "=\n",
      "<tf.Variable 'Variable:0' shape=(4, 4) dtype=float32, numpy=\n",
      "array([[ 3.3016677 , -2.169341  ,  1.7874216 ,  1.3473891 ],\n",
      "       [ 1.9178773 , -0.21830502,  0.9610931 ,  1.1830982 ],\n",
      "       [-1.4391305 , -0.9218847 ,  1.460688  ,  3.5641859 ],\n",
      "       [-0.4437298 , -0.6755494 ,  0.30157465,  1.2008858 ]],\n",
      "      dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "# Modifying the values of a variable tensor\n",
    "new_value = tf.random.normal(shape=(4,4))\n",
    "print(\"Before assign new values to 'a'\")\n",
    "print(a) \n",
    "a.assign(new_value)\n",
    "print(\"\\n\\n\\nAfter assign new values to 'a'\")\n",
    "print(a)\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        assert a[i,j] == new_value[i,j]\n",
    "        \n",
    "added_value = tf.random.normal(shape=(4,4))\n",
    "a.assign_add(added_value)\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        assert a[i,j] == new_value[i,j] + added_value[i,j]\n",
    "print(\"+\")\n",
    "print(added_value)\n",
    "print(\"=\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2be0d0cd-661c-48ed-bb74-22331635a238",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = tf.random.uniform(shape=(4, 4), minval=1, maxval=3)\n",
    "b = tf.random.uniform(shape=(4, 4), minval=1, maxval=3)\n",
    "\n",
    "c = a + b\n",
    "d = tf.square(c)\n",
    "e = tf.exp(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f7190769-dac6-4c4f-8f5b-d6fdf8b76fb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.2064722 2.0850587 1.0006375 2.6956894]\n",
      " [1.8493769 2.0728178 1.0529134 2.1878812]\n",
      " [2.4826326 1.6422379 1.5868051 1.6302793]\n",
      " [2.0928214 2.4972944 1.6231384 1.4282496]], shape=(4, 4), dtype=float32)\n",
      "\n",
      "\n",
      "\n",
      "tf.Tensor(\n",
      "[[2.8696568 2.3708205 2.2333405 1.4252715]\n",
      " [2.242848  1.6381156 2.1286764 1.0038776]\n",
      " [1.5761395 1.4440484 2.0620327 1.4767125]\n",
      " [1.3240621 2.8755388 2.8651388 2.1271987]], shape=(4, 4), dtype=float32)\n",
      "\n",
      "\n",
      "\n",
      "tf.Tensor(\n",
      "[[4.076129  4.455879  3.233978  4.120961 ]\n",
      " [4.092225  3.7109334 3.1815898 3.1917589]\n",
      " [4.058772  3.0862863 3.6488378 3.1069918]\n",
      " [3.4168835 5.3728333 4.4882774 3.5554483]], shape=(4, 4), dtype=float32)\n",
      "\n",
      "\n",
      "\n",
      "tf.Tensor(\n",
      "[[16.614828 19.85486  10.458614 16.98232 ]\n",
      " [16.746305 13.771027 10.122514 10.187325]\n",
      " [16.47363   9.525163 13.314017  9.653398]\n",
      " [11.675093 28.867336 20.144634 12.641212]], shape=(4, 4), dtype=float32)\n",
      "\n",
      "\n",
      "\n",
      "tf.Tensor(\n",
      "[[1.6433428e+07 4.1962010e+08 3.4843238e+04 2.3731666e+07]\n",
      " [1.8742522e+07 9.5649100e+05 2.4897277e+04 2.6564326e+04]\n",
      " [1.4269443e+07 1.3700158e+04 6.0562562e+05 1.5574613e+04]\n",
      " [1.1760569e+05 3.4429034e+12 5.6066502e+08 3.0903581e+05]], shape=(4, 4), dtype=float32)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(\"\\n\\n\")\n",
    "print(b)\n",
    "print(\"\\n\\n\")\n",
    "print(c)\n",
    "print(\"\\n\\n\")\n",
    "print(d)\n",
    "print(\"\\n\\n\")\n",
    "print(e)\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "058fedf8-65c7-4134-bc01-01cf27172294",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-1.0527482   0.75779563]\n",
      " [-1.6400291   1.9951824 ]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.37962675 -1.3797079 ]\n",
      " [ 0.61301965  1.7622368 ]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1.1191046 1.5741183]\n",
      " [1.7508537 2.6619976]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.9407058   0.48140958]\n",
      " [-0.9367025   0.7495057 ]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Gradients\n",
    "\n",
    "a = tf.random.normal(shape=(2, 2))\n",
    "print(a)\n",
    "b = tf.random.normal(shape=(2, 2))\n",
    "print(b)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(a)  # Start recording the history of operations applied to `a`\n",
    "    c = tf.sqrt(tf.square(a) + tf.square(b))  # Do some math using `a`\n",
    "    print(c)\n",
    "    # What's the gradient of `c` with respect to `a`?\n",
    "    dc_da = tape.gradient(c, a)\n",
    "    print(dc_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f20075dc-ccd1-45ec-91eb-d53673c48c44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.9407058   0.48140958]\n",
      " [-0.9367025   0.7495057 ]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(a)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    c = tf.sqrt(tf.square(a) + tf.square(b))\n",
    "    dc_da = tape.gradient(c, a)\n",
    "    print(dc_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bbe17a99-0f55-4aaf-991c-6a1206bd6d5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.10282564 0.48804766]\n",
      " [0.07001638 0.1646287 ]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as outer_tape:\n",
    "    with tf.GradientTape() as tape:\n",
    "        c = tf.sqrt(tf.square(a) + tf.square(b))\n",
    "        dc_da = tape.gradient(c, a)\n",
    "    d2c_da2 = outer_tape.gradient(dc_da, a)\n",
    "    print(d2c_da2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d5e276e0-f9f8-47bb-a7b5-c24ce0627f1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    \"\"\"y = w.x + b\"\"\"\n",
    "\n",
    "    def __init__(self, units=32):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "\n",
    "# Instantiate our layer.\n",
    "linear_layer = Linear(4)\n",
    "\n",
    "# This will also call `build(input_shape)` and create the weights.\n",
    "y = linear_layer(tf.ones((2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6210844f-dfe7-494c-a79a-0e6d7dfe3325",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'linear/Variable:0' shape=(2, 4) dtype=float32, numpy=\n",
      "array([[-0.00656738, -0.05906534, -0.01739354, -0.01835443],\n",
      "       [ 0.06118776,  0.05847284, -0.02498482,  0.0067023 ]],\n",
      "      dtype=float32)>\n",
      "<tf.Variable 'linear/Variable:0' shape=(4,) dtype=float32, numpy=array([-0.02224028,  0.05659447, -0.00264184, -0.01528305], dtype=float32)>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(linear_layer.w)\n",
    "print(linear_layer.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4adebab7-0c40-43d0-b48b-ae3dd4e26a3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 1s 0us/step\n",
      "Step: 0 Loss: 2.280843496322632\n",
      "Step: 100 Loss: 2.1765356063842773\n",
      "Step: 200 Loss: 2.0470731258392334\n",
      "Step: 300 Loss: 1.9574905633926392\n",
      "Step: 400 Loss: 1.8988059759140015\n",
      "Step: 500 Loss: 1.9319863319396973\n",
      "Step: 600 Loss: 1.8085181713104248\n",
      "Step: 700 Loss: 1.735289454460144\n",
      "Step: 800 Loss: 1.6278748512268066\n",
      "Step: 900 Loss: 1.5055415630340576\n"
     ]
    }
   ],
   "source": [
    "# Prepare a dataset.\n",
    "(x_train, y_train), _ = keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train)\n",
    ")\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Instantiate our linear layer (defined above) with 10 units.\n",
    "linear_layer = Linear(10)\n",
    "\n",
    "# Instantiate a logistic loss function that expects integer targets.\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "# Iterate over the batches of the dataset.\n",
    "for step, (x, y) in enumerate(dataset):\n",
    "    # Open a GradientTape.\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass.\n",
    "        logits = linear_layer(x)\n",
    "\n",
    "        # Loss value for this batch.\n",
    "        loss = loss_fn(y, logits)\n",
    "\n",
    "    # Get gradients of the loss wrt the weights.\n",
    "    gradients = tape.gradient(loss, linear_layer.trainable_weights)\n",
    "\n",
    "    # Update the weights of our linear layer.\n",
    "    optimizer.apply_gradients(zip(gradients, linear_layer.trainable_weights))\n",
    "\n",
    "    # Logging.\n",
    "    if step % 100 == 0:\n",
    "        print(\"Step:\", step, \"Loss:\", float(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5f10b608-505d-4ed1-9b68-b6de70db1bbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2.]\n",
      "[4. 4.]\n"
     ]
    }
   ],
   "source": [
    "class ComputeSum(keras.layers.Layer):\n",
    "    \"\"\"Returns the sum of the inputs.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        # Create a non-trainable weight.\n",
    "        self.total = self.add_weight(\n",
    "            initializer=\"zeros\", shape=(input_dim,), trainable=False\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
    "        return self.total\n",
    "\n",
    "\n",
    "my_sum = ComputeSum(2)\n",
    "x = tf.ones((2, 2))\n",
    "\n",
    "y = my_sum(x)\n",
    "print(y.numpy())  # [2. 2.]\n",
    "\n",
    "y = my_sum(x)\n",
    "print(y.numpy())  # [4. 4.]\n",
    "\n",
    "assert my_sum.weights == [my_sum.total]\n",
    "assert my_sum.non_trainable_weights == [my_sum.total]\n",
    "assert my_sum.trainable_weights == []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8c5ca6cd-770a-478c-b23a-0ab32f89b2fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's reuse the Linear class\n",
    "# with a `build` method that we defined above.\n",
    "\n",
    "\n",
    "class MLP(keras.layers.Layer):\n",
    "    \"\"\"Simple stack of Linear layers.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.linear_2 = Linear(32)\n",
    "        self.linear_3 = Linear(10)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return self.linear_3(x)\n",
    "\n",
    "\n",
    "mlp = MLP()\n",
    "\n",
    "# The first call to the `mlp` object will create the weights.\n",
    "y = mlp(tf.ones(shape=(3, 64)))\n",
    "\n",
    "# Weights are recursively tracked.\n",
    "assert len(mlp.weights) == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0db17f2f-2e4a-4adb-8e8d-a882bb4f0e24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlp = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(10),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b849f100-d452-422c-b7cc-e90e6cddca56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ActivityRegularization(keras.layers.Layer):\n",
    "    \"\"\"Layer that creates an activity sparsity regularization loss.\"\"\"\n",
    "\n",
    "    def __init__(self, rate=1e-2):\n",
    "        super().__init__()\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # We use `add_loss` to create a regularization loss\n",
    "        # that depends on the inputs.\n",
    "        self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a9f31434-8077-467d-80eb-a575d32d4c49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=0.13634712>]\n"
     ]
    }
   ],
   "source": [
    "# Let's use the loss layer in a MLP block.\n",
    "\n",
    "\n",
    "class SparseMLP(keras.layers.Layer):\n",
    "    \"\"\"Stack of Linear layers with a sparsity regularization loss.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.regularization = ActivityRegularization(1e-2)\n",
    "        self.linear_3 = Linear(10)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.regularization(x)\n",
    "        return self.linear_3(x)\n",
    "\n",
    "\n",
    "mlp = SparseMLP()\n",
    "y = mlp(tf.ones((10, 10)))\n",
    "\n",
    "print(mlp.losses)  # List containing one float32 scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "951694b5-79cb-4a7f-b107-70c0d0a9c187",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Loss: 6.851256370544434\n",
      "Step: 100 Loss: 2.5676000118255615\n",
      "Step: 200 Loss: 2.410273551940918\n",
      "Step: 300 Loss: 2.3714349269866943\n",
      "Step: 400 Loss: 2.345013380050659\n",
      "Step: 500 Loss: 2.330770254135132\n",
      "Step: 600 Loss: 2.334320306777954\n",
      "Step: 700 Loss: 2.322479724884033\n",
      "Step: 800 Loss: 2.325767755508423\n",
      "Step: 900 Loss: 2.3104195594787598\n"
     ]
    }
   ],
   "source": [
    "# Losses correspond to the *last* forward pass.\n",
    "mlp = SparseMLP()\n",
    "mlp(tf.ones((10, 10)))\n",
    "assert len(mlp.losses) == 1\n",
    "mlp(tf.ones((10, 10)))\n",
    "assert len(mlp.losses) == 1  # No accumulation.\n",
    "\n",
    "# Let's demonstrate how to use these losses in a training loop.\n",
    "\n",
    "# Prepare a dataset.\n",
    "(x_train, y_train), _ = keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train)\n",
    ")\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# A new MLP.\n",
    "mlp = SparseMLP()\n",
    "\n",
    "# Loss and optimizer.\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "for step, (x, y) in enumerate(dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass.\n",
    "        logits = mlp(x)\n",
    "\n",
    "        # External loss value for this batch.\n",
    "        loss = loss_fn(y, logits)\n",
    "\n",
    "        # Add the losses created during the forward pass.\n",
    "        loss += sum(mlp.losses)\n",
    "\n",
    "        # Get gradients of the loss wrt the weights.\n",
    "        gradients = tape.gradient(loss, mlp.trainable_weights)\n",
    "\n",
    "    # Update the weights of our linear layer.\n",
    "    optimizer.apply_gradients(zip(gradients, mlp.trainable_weights))\n",
    "\n",
    "    # Logging.\n",
    "    if step % 100 == 0:\n",
    "        print(\"Step:\", step, \"Loss:\", float(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "97f2b9f4-ec1a-4e62-9d42-44612e9a1ff3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Step: 0\n",
      "Total running accuracy so far: 0.062\n",
      "Epoch: 0 Step: 200\n",
      "Total running accuracy so far: 0.740\n",
      "Epoch: 0 Step: 400\n",
      "Total running accuracy so far: 0.820\n",
      "Epoch: 0 Step: 600\n",
      "Total running accuracy so far: 0.853\n",
      "Epoch: 0 Step: 800\n",
      "Total running accuracy so far: 0.871\n",
      "Epoch: 1 Step: 0\n",
      "Total running accuracy so far: 0.969\n",
      "Epoch: 1 Step: 200\n",
      "Total running accuracy so far: 0.942\n",
      "Epoch: 1 Step: 400\n",
      "Total running accuracy so far: 0.941\n",
      "Epoch: 1 Step: 600\n",
      "Total running accuracy so far: 0.942\n",
      "Epoch: 1 Step: 800\n",
      "Total running accuracy so far: 0.942\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a metric object\n",
    "accuracy = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# Prepare our layer, loss, and optimizer.\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(32, activation=\"relu\"),\n",
    "        keras.layers.Dense(32, activation=\"relu\"),\n",
    "        keras.layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "for epoch in range(2):\n",
    "    # Iterate over the batches of a dataset.\n",
    "    for step, (x, y) in enumerate(dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x)\n",
    "            # Compute the loss value for this batch.\n",
    "            loss_value = loss_fn(y, logits)\n",
    "\n",
    "        # Update the state of the `accuracy` metric.\n",
    "        accuracy.update_state(y, logits)\n",
    "\n",
    "        # Update the weights of the model to minimize the loss value.\n",
    "        gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "\n",
    "        # Logging the current accuracy value so far.\n",
    "        if step % 200 == 0:\n",
    "            print(\"Epoch:\", epoch, \"Step:\", step)\n",
    "            print(\"Total running accuracy so far: %.3f\" % accuracy.result())\n",
    "\n",
    "    # Reset the metric's state at the end of an epoch\n",
    "    accuracy.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a59f28fd-2591-410e-9c2f-42c8fe2ffcdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class F1Score(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"f1_score\", dtype=\"float32\", threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, dtype=dtype, **kwargs)\n",
    "        self.threshold = 0.5\n",
    "        self.true_positives = self.add_weight(\n",
    "            name=\"tp\", dtype=dtype, initializer=\"zeros\"\n",
    "        )\n",
    "        self.false_positives = self.add_weight(\n",
    "            name=\"fp\", dtype=dtype, initializer=\"zeros\"\n",
    "        )\n",
    "        self.false_negatives = self.add_weight(\n",
    "            name=\"fn\", dtype=dtype, initializer=\"zeros\"\n",
    "        )\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.math.greater_equal(y_pred, self.threshold)\n",
    "        y_true = tf.cast(y_true, tf.bool)\n",
    "        y_pred = tf.cast(y_pred, tf.bool)\n",
    "\n",
    "        true_positives = tf.cast(y_true & y_pred, self.dtype)\n",
    "        false_positives = tf.cast(~y_true & y_pred, self.dtype)\n",
    "        false_negatives = tf.cast(y_true & ~y_pred, self.dtype)\n",
    "\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, self.dtype)\n",
    "            true_positives *= sample_weight\n",
    "            false_positives *= sample_weight\n",
    "            false_negatives *= sample_weight\n",
    "\n",
    "        self.true_positives.assign_add(tf.reduce_sum(true_positives))\n",
    "        self.false_positives.assign_add(tf.reduce_sum(false_positives))\n",
    "        self.false_negatives.assign_add(tf.reduce_sum(false_negatives))\n",
    "\n",
    "    def result(self):\n",
    "        precision = self.true_positives / (self.true_positives + self.false_positives)\n",
    "        recall = self.true_positives / (self.true_positives + self.false_negatives)\n",
    "        return precision * recall * 2.0 / (precision + recall)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(0)\n",
    "        self.false_positives.assign(0)\n",
    "        self.false_negatives.assign(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4dc703fd-3316-43b2-ac0c-cf04cbf439c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate result: 0.5\n",
      "Final result: 0.6000000238418579\n"
     ]
    }
   ],
   "source": [
    "m = F1Score()\n",
    "m.update_state([0, 1, 0, 0], [0.3, 0.5, 0.8, 0.9])\n",
    "print(\"Intermediate result:\", float(m.result()))\n",
    "\n",
    "m.update_state([1, 1, 1, 1], [0.1, 0.7, 0.6, 0.0])\n",
    "print(\"Final result:\", float(m.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1afae4bf-cb4f-4228-8d2d-42e1a0a0a900",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Loss: 2.284780740737915\n",
      "Step: 100 Loss: 0.46894195675849915\n",
      "Step: 200 Loss: 0.4187391400337219\n",
      "Step: 300 Loss: 0.27640199661254883\n",
      "Step: 400 Loss: 0.3544742465019226\n",
      "Step: 500 Loss: 0.503422737121582\n",
      "Step: 600 Loss: 0.3379875719547272\n",
      "Step: 700 Loss: 0.3025365173816681\n",
      "Step: 800 Loss: 0.2821683883666992\n",
      "Step: 900 Loss: 0.07792575657367706\n"
     ]
    }
   ],
   "source": [
    "# Prepare our layer, loss, and optimizer.\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(32, activation=\"relu\"),\n",
    "        keras.layers.Dense(32, activation=\"relu\"),\n",
    "        keras.layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# Create a training step function.\n",
    "\n",
    "\n",
    "@tf.function  # Make it fast.\n",
    "def train_on_batch(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(y, logits)\n",
    "        gradients = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Prepare a dataset.\n",
    "(x_train, y_train), _ = keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train)\n",
    ")\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "for step, (x, y) in enumerate(dataset):\n",
    "    loss = train_on_batch(x, y)\n",
    "    if step % 100 == 0:\n",
    "        print(\"Step:\", step, \"Loss:\", float(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62804e2-7257-41b1-ab4a-7998bb9f8e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
